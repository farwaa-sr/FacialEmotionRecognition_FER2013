{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farwa\\AppData\\Local\\Temp\\ipykernel_14544\\2277697435.py:58: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "316/316 [==============================] - 236s 735ms/step - loss: 1.8127 - accuracy: 0.2527 - val_loss: 1.7647 - val_accuracy: 0.2891\n",
      "Epoch 2/70\n",
      "316/316 [==============================] - 212s 671ms/step - loss: 1.6695 - accuracy: 0.3433 - val_loss: 1.5688 - val_accuracy: 0.4061\n",
      "Epoch 3/70\n",
      "316/316 [==============================] - 199s 629ms/step - loss: 1.5528 - accuracy: 0.4008 - val_loss: 1.4910 - val_accuracy: 0.4367\n",
      "Epoch 4/70\n",
      "316/316 [==============================] - 204s 645ms/step - loss: 1.4908 - accuracy: 0.4284 - val_loss: 1.4232 - val_accuracy: 0.4588\n",
      "Epoch 5/70\n",
      "316/316 [==============================] - 206s 651ms/step - loss: 1.4369 - accuracy: 0.4480 - val_loss: 1.3876 - val_accuracy: 0.4724\n",
      "Epoch 6/70\n",
      "316/316 [==============================] - 196s 621ms/step - loss: 1.3960 - accuracy: 0.4651 - val_loss: 1.3590 - val_accuracy: 0.4806\n",
      "Epoch 7/70\n",
      "316/316 [==============================] - 204s 644ms/step - loss: 1.3549 - accuracy: 0.4830 - val_loss: 1.3259 - val_accuracy: 0.4955\n",
      "Epoch 8/70\n",
      "316/316 [==============================] - 204s 644ms/step - loss: 1.3230 - accuracy: 0.4980 - val_loss: 1.3042 - val_accuracy: 0.5039\n",
      "Epoch 9/70\n",
      "316/316 [==============================] - 204s 644ms/step - loss: 1.2898 - accuracy: 0.5106 - val_loss: 1.2759 - val_accuracy: 0.5149\n",
      "Epoch 10/70\n",
      "316/316 [==============================] - 207s 654ms/step - loss: 1.2702 - accuracy: 0.5192 - val_loss: 1.2581 - val_accuracy: 0.5184\n",
      "Epoch 11/70\n",
      "316/316 [==============================] - 204s 647ms/step - loss: 1.2477 - accuracy: 0.5282 - val_loss: 1.2362 - val_accuracy: 0.5250\n",
      "Epoch 12/70\n",
      "316/316 [==============================] - 199s 628ms/step - loss: 1.2208 - accuracy: 0.5415 - val_loss: 1.2376 - val_accuracy: 0.5272\n",
      "Epoch 13/70\n",
      "316/316 [==============================] - 206s 651ms/step - loss: 1.2021 - accuracy: 0.5473 - val_loss: 1.2097 - val_accuracy: 0.5388\n",
      "Epoch 14/70\n",
      "316/316 [==============================] - 204s 646ms/step - loss: 1.1815 - accuracy: 0.5547 - val_loss: 1.1941 - val_accuracy: 0.5438\n",
      "Epoch 15/70\n",
      "316/316 [==============================] - 197s 624ms/step - loss: 1.1590 - accuracy: 0.5648 - val_loss: 1.1811 - val_accuracy: 0.5455\n",
      "Epoch 16/70\n",
      "316/316 [==============================] - 205s 649ms/step - loss: 1.1359 - accuracy: 0.5756 - val_loss: 1.1671 - val_accuracy: 0.5519\n",
      "Epoch 17/70\n",
      "316/316 [==============================] - 205s 649ms/step - loss: 1.1305 - accuracy: 0.5775 - val_loss: 1.1615 - val_accuracy: 0.5582\n",
      "Epoch 18/70\n",
      "316/316 [==============================] - 200s 632ms/step - loss: 1.1080 - accuracy: 0.5871 - val_loss: 1.1453 - val_accuracy: 0.5656\n",
      "Epoch 19/70\n",
      "316/316 [==============================] - 206s 652ms/step - loss: 1.0867 - accuracy: 0.5918 - val_loss: 1.1403 - val_accuracy: 0.5667\n",
      "Epoch 20/70\n",
      "316/316 [==============================] - 210s 664ms/step - loss: 1.0741 - accuracy: 0.5946 - val_loss: 1.1378 - val_accuracy: 0.5699\n",
      "Epoch 21/70\n",
      "316/316 [==============================] - 200s 634ms/step - loss: 1.0553 - accuracy: 0.6060 - val_loss: 1.1331 - val_accuracy: 0.5713\n",
      "Epoch 22/70\n",
      "316/316 [==============================] - 205s 649ms/step - loss: 1.0383 - accuracy: 0.6117 - val_loss: 1.1285 - val_accuracy: 0.5741\n",
      "Epoch 23/70\n",
      "316/316 [==============================] - 204s 646ms/step - loss: 1.0274 - accuracy: 0.6155 - val_loss: 1.1124 - val_accuracy: 0.5784\n",
      "Epoch 24/70\n",
      "316/316 [==============================] - 200s 632ms/step - loss: 1.0091 - accuracy: 0.6252 - val_loss: 1.1038 - val_accuracy: 0.5845\n",
      "Epoch 25/70\n",
      "316/316 [==============================] - 205s 648ms/step - loss: 0.9937 - accuracy: 0.6296 - val_loss: 1.1009 - val_accuracy: 0.5850\n",
      "Epoch 26/70\n",
      "316/316 [==============================] - 210s 666ms/step - loss: 0.9813 - accuracy: 0.6333 - val_loss: 1.0975 - val_accuracy: 0.5872\n",
      "Epoch 27/70\n",
      "316/316 [==============================] - 206s 651ms/step - loss: 0.9574 - accuracy: 0.6451 - val_loss: 1.0927 - val_accuracy: 0.5876\n",
      "Epoch 28/70\n",
      "316/316 [==============================] - 253s 800ms/step - loss: 0.9429 - accuracy: 0.6520 - val_loss: 1.0873 - val_accuracy: 0.5886\n",
      "Epoch 29/70\n",
      "316/316 [==============================] - 265s 838ms/step - loss: 0.9347 - accuracy: 0.6513 - val_loss: 1.0942 - val_accuracy: 0.5917\n",
      "Epoch 30/70\n",
      "316/316 [==============================] - 258s 814ms/step - loss: 0.9170 - accuracy: 0.6644 - val_loss: 1.0960 - val_accuracy: 0.5907\n",
      "Epoch 31/70\n",
      "316/316 [==============================] - 226s 714ms/step - loss: 0.8968 - accuracy: 0.6660 - val_loss: 1.0830 - val_accuracy: 0.6006\n",
      "Epoch 32/70\n",
      "316/316 [==============================] - 204s 645ms/step - loss: 0.8823 - accuracy: 0.6746 - val_loss: 1.0838 - val_accuracy: 0.5995\n",
      "Epoch 33/70\n",
      "316/316 [==============================] - 191s 603ms/step - loss: 0.8656 - accuracy: 0.6818 - val_loss: 1.0685 - val_accuracy: 0.6069\n",
      "Epoch 34/70\n",
      "316/316 [==============================] - 215s 681ms/step - loss: 0.8582 - accuracy: 0.6850 - val_loss: 1.0735 - val_accuracy: 0.6023\n",
      "Epoch 35/70\n",
      "316/316 [==============================] - 221s 700ms/step - loss: 0.8347 - accuracy: 0.6912 - val_loss: 1.0694 - val_accuracy: 0.6087\n",
      "Epoch 36/70\n",
      "316/316 [==============================] - 212s 670ms/step - loss: 0.8094 - accuracy: 0.7015 - val_loss: 1.0867 - val_accuracy: 0.6038\n",
      "Epoch 37/70\n",
      "316/316 [==============================] - 222s 704ms/step - loss: 0.8071 - accuracy: 0.7050 - val_loss: 1.0794 - val_accuracy: 0.6113\n",
      "Epoch 38/70\n",
      "316/316 [==============================] - 225s 711ms/step - loss: 0.7899 - accuracy: 0.7105 - val_loss: 1.0776 - val_accuracy: 0.6070\n",
      "Epoch 39/70\n",
      "316/316 [==============================] - 234s 738ms/step - loss: 0.7721 - accuracy: 0.7155 - val_loss: 1.0779 - val_accuracy: 0.6060\n",
      "Epoch 40/70\n",
      "316/316 [==============================] - 219s 693ms/step - loss: 0.7639 - accuracy: 0.7202 - val_loss: 1.0698 - val_accuracy: 0.6133\n",
      "Epoch 41/70\n",
      "316/316 [==============================] - 208s 659ms/step - loss: 0.7524 - accuracy: 0.7204 - val_loss: 1.0724 - val_accuracy: 0.6115\n",
      "Epoch 42/70\n",
      "316/316 [==============================] - 198s 628ms/step - loss: 0.7361 - accuracy: 0.7289 - val_loss: 1.0868 - val_accuracy: 0.6124\n",
      "Epoch 43/70\n",
      "316/316 [==============================] - 199s 629ms/step - loss: 0.7125 - accuracy: 0.7387 - val_loss: 1.0813 - val_accuracy: 0.6097\n",
      "Epoch 44/70\n",
      "316/316 [==============================] - 197s 623ms/step - loss: 0.7014 - accuracy: 0.7447 - val_loss: 1.0857 - val_accuracy: 0.6126\n",
      "Epoch 45/70\n",
      "316/316 [==============================] - 192s 606ms/step - loss: 0.6820 - accuracy: 0.7520 - val_loss: 1.0706 - val_accuracy: 0.6193\n",
      "Epoch 46/70\n",
      "316/316 [==============================] - 197s 622ms/step - loss: 0.6798 - accuracy: 0.7539 - val_loss: 1.0793 - val_accuracy: 0.6196\n",
      "Epoch 47/70\n",
      "316/316 [==============================] - 199s 631ms/step - loss: 0.6591 - accuracy: 0.7607 - val_loss: 1.0768 - val_accuracy: 0.6201\n",
      "Epoch 48/70\n",
      "316/316 [==============================] - 193s 612ms/step - loss: 0.6374 - accuracy: 0.7663 - val_loss: 1.0937 - val_accuracy: 0.6155\n",
      "Epoch 49/70\n",
      "316/316 [==============================] - 199s 628ms/step - loss: 0.6219 - accuracy: 0.7746 - val_loss: 1.0962 - val_accuracy: 0.6176\n",
      "Epoch 50/70\n",
      "316/316 [==============================] - 196s 621ms/step - loss: 0.6092 - accuracy: 0.7809 - val_loss: 1.0918 - val_accuracy: 0.6155\n",
      "Epoch 51/70\n",
      "316/316 [==============================] - 193s 610ms/step - loss: 0.5938 - accuracy: 0.7838 - val_loss: 1.1056 - val_accuracy: 0.6204\n",
      "Epoch 52/70\n",
      "316/316 [==============================] - 202s 638ms/step - loss: 0.5817 - accuracy: 0.7930 - val_loss: 1.1054 - val_accuracy: 0.6186\n",
      "Epoch 53/70\n",
      "316/316 [==============================] - 195s 617ms/step - loss: 0.5709 - accuracy: 0.7962 - val_loss: 1.1202 - val_accuracy: 0.6166\n",
      "Epoch 54/70\n",
      "316/316 [==============================] - 195s 618ms/step - loss: 0.5589 - accuracy: 0.7975 - val_loss: 1.1118 - val_accuracy: 0.6191\n",
      "Epoch 55/70\n",
      "316/316 [==============================] - 197s 623ms/step - loss: 0.5413 - accuracy: 0.8046 - val_loss: 1.1241 - val_accuracy: 0.6194\n",
      "Epoch 56/70\n",
      "316/316 [==============================] - 195s 617ms/step - loss: 0.5278 - accuracy: 0.8110 - val_loss: 1.1265 - val_accuracy: 0.6208\n",
      "Epoch 57/70\n",
      "316/316 [==============================] - 196s 619ms/step - loss: 0.5222 - accuracy: 0.8111 - val_loss: 1.1346 - val_accuracy: 0.6229\n",
      "Epoch 58/70\n",
      "316/316 [==============================] - 196s 620ms/step - loss: 0.5187 - accuracy: 0.8120 - val_loss: 1.1212 - val_accuracy: 0.6232\n",
      "Epoch 59/70\n",
      "316/316 [==============================] - 194s 614ms/step - loss: 0.4962 - accuracy: 0.8220 - val_loss: 1.1357 - val_accuracy: 0.6222\n",
      "Epoch 60/70\n",
      "316/316 [==============================] - 195s 617ms/step - loss: 0.4833 - accuracy: 0.8283 - val_loss: 1.1297 - val_accuracy: 0.6217\n",
      "Epoch 61/70\n",
      "316/316 [==============================] - 199s 629ms/step - loss: 0.4685 - accuracy: 0.8308 - val_loss: 1.1331 - val_accuracy: 0.6257\n",
      "Epoch 62/70\n",
      "316/316 [==============================] - 194s 614ms/step - loss: 0.4684 - accuracy: 0.8313 - val_loss: 1.1359 - val_accuracy: 0.6239\n",
      "Epoch 63/70\n",
      "316/316 [==============================] - 197s 622ms/step - loss: 0.4594 - accuracy: 0.8347 - val_loss: 1.1554 - val_accuracy: 0.6271\n",
      "Epoch 64/70\n",
      "316/316 [==============================] - 197s 624ms/step - loss: 0.4377 - accuracy: 0.8412 - val_loss: 1.1760 - val_accuracy: 0.6243\n",
      "Epoch 65/70\n",
      "316/316 [==============================] - 193s 611ms/step - loss: 0.4356 - accuracy: 0.8437 - val_loss: 1.1663 - val_accuracy: 0.6243\n",
      "Epoch 66/70\n",
      "316/316 [==============================] - 199s 631ms/step - loss: 0.4200 - accuracy: 0.8487 - val_loss: 1.1622 - val_accuracy: 0.6246\n",
      "Epoch 67/70\n",
      "316/316 [==============================] - 197s 622ms/step - loss: 0.4141 - accuracy: 0.8494 - val_loss: 1.1890 - val_accuracy: 0.6239\n",
      "Epoch 68/70\n",
      "316/316 [==============================] - 191s 605ms/step - loss: 0.4004 - accuracy: 0.8546 - val_loss: 1.1837 - val_accuracy: 0.6314\n",
      "Epoch 69/70\n",
      "316/316 [==============================] - 197s 624ms/step - loss: 0.3998 - accuracy: 0.8541 - val_loss: 1.1944 - val_accuracy: 0.6264\n",
      "Epoch 70/70\n",
      "316/316 [==============================] - 197s 624ms/step - loss: 0.3794 - accuracy: 0.8638 - val_loss: 1.1794 - val_accuracy: 0.6257\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import cv2  # OpenCV for image processing\n",
    "from keras.models import Sequential  # Sequential model for building the neural network\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten  # Layers used in the model\n",
    "from keras.optimizers import Adam  # Adam optimizer for model optimization\n",
    "from keras.preprocessing.image import ImageDataGenerator  # ImageDataGenerator for image preprocessing\n",
    "\n",
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1] range\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all training images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        'C:\\\\Users\\\\farwa\\\\.virtualenvs\\\\.virtualenvs\\\\EmotionCNN\\\\train',  # Directory containing training images\n",
    "        target_size=(48, 48),  # Resize images to (48, 48)\n",
    "        batch_size=64,  # Number of images in each batch\n",
    "        color_mode=\"grayscale\",  # Convert images to grayscale\n",
    "        class_mode='categorical')  # Generate categorical labels\n",
    "\n",
    "# Preprocess all validation images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        'test',  # Directory containing validation images\n",
    "        target_size=(48, 48),  # Resize images to (48, 48)\n",
    "        batch_size=64,  # Number of images in each batch\n",
    "        color_mode=\"grayscale\",  # Convert images to grayscale\n",
    "        class_mode='categorical')  # Generate categorical labels\n",
    "\n",
    "# Update the number of images in train_generator and validation_generator\n",
    "train_generator.samples = 20269\n",
    "validation_generator.samples = 7178\n",
    "\n",
    "# create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "# Add convolutional layers with ReLU activation and pooling layers for downsampling\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))  # 32 filters, 3x3 kernel, input shape (48, 48, 1)\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))  # 64 filters, 3x3 kernel\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with pool size (2, 2)\n",
    "emotion_model.add(Dropout(0.25))  # Dropout layer with a dropout rate of 0.25\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))  # 128 filters, 3x3 kernel\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with pool size (2, 2)\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))  # 128 filters, 3x3 kernel\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with pool size (2, 2)\n",
    "emotion_model.add(Dropout(0.25))  # Dropout layer with a dropout rate of 0.25\n",
    "\n",
    "emotion_model.add(Flatten())  # Flatten the output\n",
    "emotion_model.add(Dense(1024, activation='relu'))  # Fully connected layer with 1024 units and ReLU activation\n",
    "emotion_model.add(Dropout(0.5))  # Dropout layer with a dropout rate of 0.5\n",
    "emotion_model.add(Dense(7, activation='softmax'))  # Final fully connected layer with 7 units for the 7 emotion categories and softmax activation\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)  # Disable OpenCL optimization for OpenCV\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "# Compile the model with categorical cross-entropy loss, Adam optimizer with learning rate 0.0001 and decay 1e-6, and accuracy metric\n",
    "\n",
    "# Train the neural network/model\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,  # Training data generator\n",
    "        steps_per_epoch=20269 // 64,  # Number of steps (batches) per epoch\n",
    "        epochs=70,  # Number of training epochs\n",
    "        validation_data=validation_generator,  # Validation data generator\n",
    "        validation_steps=7178 // 64)  # Number of steps (batches) for validation\n",
    "\n",
    "# save model structure in JSON file\n",
    "model_json = emotion_model.to_json()  # Convert the model structure to JSON format\n",
    "with open(\"emotion_model_redu.json\", \"w\") as json_file:  # Open a file to write the JSON structure\n",
    "    json_file.write(model_json)  # Write the JSON structure to the file\n",
    "\n",
    "# save trained model weights in .h5 file\n",
    "emotion_model.save_weights('emotion_model_redu.h5')  # Save the trained model weights to an .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 6781 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farwa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\farwa\\AppData\\Local\\Temp\\ipykernel_27764\\1196048397.py:54: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/99\n",
      "169/597 [=======>......................] - ETA: 1:34 - loss: 1.8475 - accuracy: 0.2356"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# import required packages\n",
    "import cv2  # OpenCV for image processing\n",
    "from keras.models import Sequential  # Sequential model for building the neural network\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten  # Layers used in the model\n",
    "from keras.optimizers import Adam  # Adam optimizer for model optimization\n",
    "from keras.preprocessing.image import ImageDataGenerator  # ImageDataGenerator for image preprocessing\n",
    "\n",
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1] range\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all training images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        'C:\\\\Users\\\\farwa\\\\.virtualenvs\\\\.virtualenvs\\\\EmotionCNN\\\\train',  # Directory containing training images\n",
    "        target_size=(48, 48),  # Resize images to (48, 48)\n",
    "        batch_size=32,  # Number of images in each batch\n",
    "        color_mode=\"grayscale\",  # Convert images to grayscale\n",
    "        class_mode='categorical')  # Generate categorical labels\n",
    "\n",
    "# Preprocess all validation images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        'test',  # Directory containing validation images\n",
    "        target_size=(48, 48),  # Resize images to (48, 48)\n",
    "        batch_size=32,  # Number of images in each batch\n",
    "        color_mode=\"grayscale\",  # Convert images to grayscale\n",
    "        class_mode='categorical')  # Generate categorical labels\n",
    "\n",
    "# create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "# Add convolutional layers with ReLU activation and pooling layers for downsampling\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))  # 32 filters, 3x3 kernel, input shape (48, 48, 1)\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))  # 64 filters, 3x3 kernel\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with pool size (2, 2)\n",
    "emotion_model.add(Dropout(0.35))  # Dropout layer with a dropout rate of 0.30\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))  # 128 filters, 3x3 kernel\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with pool size (2, 2)\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))  # 128 filters, 3x3 kernel\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with pool size (2, 2)\n",
    "emotion_model.add(Dropout(0.35))  # Dropout layer with a dropout rate of 0.30\n",
    "\n",
    "emotion_model.add(Flatten())  # Flatten the output\n",
    "emotion_model.add(Dense(1024, activation='relu'))  # Fully connected layer with 1024 units and ReLU activation\n",
    "emotion_model.add(Dropout(0.60))  # Dropout layer with a dropout rate of 0.55\n",
    "emotion_model.add(Dense(7, activation='softmax'))  # Final fully connected layer with 7 units for the 7 emotion categories and softmax activation\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)  # Disable OpenCL optimization for OpenCV\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "# Compile the model with categorical cross-entropy loss, Adam optimizer with learning rate 0.0001 and decay 1e-6, and accuracy metric\n",
    "\n",
    "# Train the neural network/model\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,  # Training data generator\n",
    "        steps_per_epoch=19126 // 32,  # Number of steps (batches) per epoch\n",
    "        epochs=99,  # Number of training epochs\n",
    "        validation_data=validation_generator,  # Validation data generator\n",
    "        validation_steps=6781 // 32)  # Number of steps (batches) for validation\n",
    "\n",
    "# save model structure in JSON file\n",
    "model_json = emotion_model.to_json()  # Convert the model structure to JSON format\n",
    "with open(\"emotion_model_redu_v2.json\", \"w\") as json_file:  # Open a file to write the JSON structure\n",
    "    json_file.write(model_json)  # Write the JSON structure to the file\n",
    "\n",
    "# save trained model weights in .h5 file\n",
    "emotion_model.save_weights('emotion_model_redu_v2.h5')  # Save the trained model weights to an .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the predicted probabilities for the validation dataset\n",
    "validation_predictions = emotion_model.predict_generator(validation_generator)\n",
    "\n",
    "# Get the predicted labels by taking the index of the highest probability\n",
    "validation_predicted_labels = np.argmax(validation_predictions, axis=1)\n",
    "\n",
    "# Get the true labels from the validation generator\n",
    "validation_true_labels = validation_generator.classes\n",
    "\n",
    "# Get the class labels (emotions) from the validation generator\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "\n",
    "# Initialize a dictionary to store the counts and accuracies for each emotion\n",
    "emotion_counts = {}\n",
    "emotion_accuracies = {}\n",
    "\n",
    "# Initialize the counts for each emotion to 0\n",
    "for label in class_labels:\n",
    "    emotion_counts[label] = 0\n",
    "\n",
    "# Iterate over the true labels and predicted labels\n",
    "for true_label, predicted_label in zip(validation_true_labels, validation_predicted_labels):\n",
    "    true_emotion = class_labels[true_label]\n",
    "    predicted_emotion = class_labels[predicted_label]\n",
    "    \n",
    "    # Increment the count for the true emotion\n",
    "    emotion_counts[true_emotion] += 1\n",
    "    \n",
    "    # Check if the predicted emotion matches the true emotion\n",
    "    if predicted_emotion == true_emotion:\n",
    "        if true_emotion not in emotion_accuracies:\n",
    "            emotion_accuracies[true_emotion] = 0\n",
    "        emotion_accuracies[true_emotion] += 1\n",
    "\n",
    "# Calculate the accuracies for each emotion\n",
    "for emotion in emotion_accuracies:\n",
    "    emotion_accuracies[emotion] /= emotion_counts[emotion]\n",
    "\n",
    "# Print the accuracies for each emotion\n",
    "for emotion in emotion_accuracies:\n",
    "    print(f\"Accuracy for {emotion}: {emotion_accuracies[emotion]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emotion_model_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Get training and validation loss from the model training history\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m training_loss \u001b[39m=\u001b[39m emotion_model_info\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m validation_loss \u001b[39m=\u001b[39m emotion_model_info\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[39m# Get training and validation accuracy from the model training history\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emotion_model_info' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and validation loss from the model training history\n",
    "training_loss = emotion_model_info.history['loss']\n",
    "validation_loss = emotion_model_info.history['val_loss']\n",
    "\n",
    "# Get training and validation accuracy from the model training history\n",
    "training_accuracy = emotion_model_info.history['accuracy']\n",
    "validation_accuracy = emotion_model_info.history['val_accuracy']\n",
    "\n",
    "# Create a figure with two subplots for loss and accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot training and validation loss\n",
    "ax1.plot(training_loss, label='Training Loss')\n",
    "ax1.plot(validation_loss, label='Validation Loss')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "ax2.plot(training_accuracy, label='Training Accuracy')\n",
    "ax2.plot(validation_accuracy, label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the layout of the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# import required packages\n",
    "import cv2  # OpenCV for image processing\n",
    "from keras.models import Sequential  # Sequential model for building the neural network\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten  # Layers used in the model\n",
    "from keras.optimizers import Adam  # Adam optimizer for model optimization\n",
    "from keras.preprocessing.image import ImageDataGenerator  # ImageDataGenerator for image preprocessing\n",
    "\n",
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1] range\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all training images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        'C:\\\\Users\\\\farwa\\\\.virtualenvs\\\\.virtualenvs\\\\EmotionCNN\\\\train',  \n",
    "        target_size=(48, 48),  \n",
    "        batch_size=32,  \n",
    "        color_mode=\"grayscale\",  \n",
    "        class_mode='categorical')  \n",
    "\n",
    "# Preprocess all validation images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        'test',  \n",
    "        target_size=(48, 48),  \n",
    "        batch_size=32, \n",
    "        color_mode=\"grayscale\",  \n",
    "        class_mode='categorical') \n",
    "\n",
    "# create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "# Add convolutional layers with ReLU activation and pooling layers for downsampling\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))  \n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))  \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "emotion_model.add(Dropout(0.35))  \n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu')) \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))  \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "emotion_model.add(Dropout(0.35)) \n",
    "\n",
    "emotion_model.add(Flatten())  \n",
    "emotion_model.add(Dense(1024, activation='relu'))  \n",
    "emotion_model.add(Dropout(0.60))  \n",
    "emotion_model.add(Dense(7, activation='softmax'))  \n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)  \n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "# Compile the model with categorical cross-entropy loss, Adam optimizer with learning rate 0.0001 and decay 1e-6, and accuracy metric\n",
    "\n",
    "# Train the neural network/model\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,  \n",
    "        steps_per_epoch=19126 // 32,  \n",
    "        epochs=100,  \n",
    "        validation_data=validation_generator,  \n",
    "        validation_steps=6781 // 32)  \n",
    "\n",
    "# save model structure in JSON file\n",
    "model_json = emotion_model.to_json()  \n",
    "with open(\"emotion_model_redu_v2.json\", \"w\") as json_file:  \n",
    "    json_file.write(model_json)  \n",
    "\n",
    "# save trained model weights in .h5 file\n",
    "emotion_model.save_weights('emotion_model_redu_v2.h5')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
